---
title : "[Python/NLP] 용어 사전 만들기"
date : 2020.09.10
categories : NLP
tags:
- NLP
- dictionary
---

수정중

한국어로 자연어처리 할 때 가장 불편했던 점은 띄어쓰기 단위로 단어를 인식할 수 없다는 점이다.


## BERT의 한국어 처리 문제점

[BERT Github](https://github.com/google-research/bert/blob/master/multilingual.md)   

토큰 화를 위해 110k 공유 WordPiece 어휘를 사용합니다. 단어 수는 데이터와 같은 방식으로 가중치가 부여되므로 리소스가 적은 언어는 일부 요인에 의해 가중치가 높아집니다. 우리는 의도적으로 입력 언어를 나타내는 마커를 사용하지 않습니다 (제로 샷 훈련이 작동 할 수 있도록).
중국어 (및 일본어 간지 및 한국어 한자)에는 공백 문자가 없기 때문에 WordPiece를 적용하기 전에 CJK 유니 코드 범위의 모든 문자 주위에 공백을 추가합니다. 이것은 중국어가 효과적으로 문자 토큰 화됨을 의미합니다. CJK 유니 코드 블록에는 중국어 원본 문자 만 포함되며 다른 모든 언어와 마찬가지로 공백 + WordPiece로 토큰 화 된 한글 또는 가타카나 / 히라가나 일본어는 포함되지 않습니다.
다른 모든 언어의 경우 (a) 소문자 + 악센트 제거, (b) 구두점 분할, (c) 공백 토큰 화와 같이 영어와 동일한 레시피를 적용합니다. 우리는 악센트 표시가 일부 언어에서 상당한 의미를 가지고 있음을 이해하지만 효과적인 어휘 감소의 이점이이를 보완한다고 느꼈습니다. 일반적으로 BERT의 강력한 컨텍스트 모델은 강조 표시를 제거하여 발생하는 모호성을 보완해야합니다.


# koBert

[KoBERT Github](https://github.com/SKTBrain/KoBERT#why)
